{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mnist_seq2seq.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aliborji/A-Collection-of-important-tasks-in-pytorch/blob/master/mnist_seq2seq.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dOZRSuUMEEvd"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision.datasets import MNIST\n",
        "from torch.utils.data import DataLoader\n",
        "from multiprocessing import cpu_count\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "ROOT = './'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3GV6o1WEZHI"
      },
      "source": [
        "def get_data(batch_size=64):\n",
        "    class Dataset:\n",
        "        def __init__(self, images, targets):\n",
        "            self.images = images\n",
        "            self.targets = targets\n",
        "\n",
        "        def __getitem__(self, idx):\n",
        "            return self.images[idx], self.targets[idx]\n",
        "\n",
        "        def __len__(self):\n",
        "            return self.images.shape[0]\n",
        "\n",
        "    def split(data, valid_pct=0.3):\n",
        "        cutoff = int((1 - valid_pct) * data.shape[0])\n",
        "        train = data[:cutoff]\n",
        "        valid = data[cutoff:]\n",
        "        return train, valid\n",
        "\n",
        "    mnist = MNIST(ROOT, download=True)\n",
        "    train_images, valid_images = split(mnist.data.float() / 255)\n",
        "    train_labels, valid_labels = split(mnist.targets)\n",
        "    train_ds = Dataset(train_images, train_labels)\n",
        "    valid_ds = Dataset(valid_images, valid_labels)\n",
        "    num_workers = cpu_count()   \n",
        "    train_dl = DataLoader(train_ds,\n",
        "                          batch_size=batch_size,\n",
        "                          shuffle=True,\n",
        "                          num_workers=num_workers,\n",
        "                          pin_memory=True)\n",
        "    valid_dl = DataLoader(valid_ds,\n",
        "                          batch_size=batch_size*2,\n",
        "                          shuffle=False,\n",
        "                          num_workers=num_workers,\n",
        "                          pin_memory=True)\n",
        "    \n",
        "    return train_dl, valid_dl"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iQ2gXVurJ8Ei"
      },
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "\n",
        "    def __init__(self,\n",
        "                 dim,            # dimension of attention layers\n",
        "                 src_vocab_size, # used to determine token embeddings\n",
        "                 tgt_vocab_size, # ...\n",
        "                 src_max_len,    # used to determine positional embeddings\n",
        "                 tgt_max_len,    # ...\n",
        "                 device,         # the device hosting the computation\n",
        "                 num_heads=1,    # number of heads per attention layer (divides into dim)\n",
        "                 num_layers=1,   # number of encoder & decoder layers\n",
        "                 expand=1,       # the factor by which to expand dim for internal attention layer\n",
        "                 p=0.1):         # dropout probability\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(dim, src_vocab_size, src_max_len, num_heads, num_layers, expand, p)\n",
        "        self.decoder = Decoder(dim, tgt_vocab_size, tgt_max_len, num_heads, num_layers, expand, p)\n",
        "        self.device = device\n",
        "\n",
        "        \n",
        "    def __len__(self):\n",
        "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
        "\n",
        "\n",
        "    def make_src_mask(self, src):        \n",
        "        # src = (N, len)\n",
        "        # src_mask = (N, 1, 1, len)\n",
        "        src_mask = (src != self.pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "        return src_mask.to(src.device)\n",
        "\n",
        "    \n",
        "    def make_tgt_mask(self, tgt):     \n",
        "        # tgt = (N, len)\n",
        "        \n",
        "        # tgt_sub_mask = (len, len)\n",
        "        tgt_len = tgt.shape[1]\n",
        "        tgt_sub_mask = torch.tril(torch.ones((tgt_len, tgt_len)))\n",
        "        tgt_sub_mask = tgt_sub_mask.bool().to(tgt.device)\n",
        "\n",
        "        return tgt_sub_mask\n",
        "\n",
        "    \n",
        "    def forward(self, src, tgt):\n",
        "        src_mask = None\n",
        "        tgt_mask = self.make_tgt_mask(tgt)\n",
        "\n",
        "        enc_out = self.encoder(src, src_mask)\n",
        "        dec_out = self.decoder(tgt, enc_out, src_mask, tgt_mask)\n",
        "        return F.linear(dec_out, self.decoder.embedding.tok_embedding.weight)\n",
        "\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "\n",
        "    def __init__(self, dim, vocab_size, max_len, num_heads, num_layers, expand, p):\n",
        "        super().__init__()\n",
        "        self.embedding = PosEmbedding(dim, max_len, p)\n",
        "        self.layers = nn.ModuleList(\n",
        "            [TransformerBlock(dim, num_heads, expand, p)\n",
        "             for _ in range(num_layers)]\n",
        "        )\n",
        "\n",
        "    \n",
        "    def forward(self, src, mask=None):\n",
        "        # src = (N, seq_len)\n",
        "        out = self.embedding(src)\n",
        "        for layer in self.layers:\n",
        "            out = layer(out, out, out, mask)       \n",
        "        # out = (N, seq_len, dim)\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "\n",
        "    def __init__(self, dim, vocab_size, max_len, num_heads, num_layers, expand, p):\n",
        "        super().__init__()\n",
        "        self.embedding = Embedding(dim, vocab_size, max_len, p)\n",
        "        self.layers = nn.ModuleList(\n",
        "            [DecoderBlock(dim, num_heads, expand, p) for _ in range(num_layers)]\n",
        "        )\n",
        "\n",
        "    \n",
        "    def forward(self, tgt, enc_out, src_mask, tgt_mask):\n",
        "        # tgt = (N, len)\n",
        "        # enc_out = (N, len, dim)\n",
        "        \n",
        "        # tgt_out = (N, len, dim)\n",
        "        tgt_out = self.embedding(tgt)\n",
        "        for layer in self.layers:\n",
        "            tgt_out = layer(tgt_out, enc_out, enc_out, src_mask, tgt_mask)\n",
        "        \n",
        "        return tgt_out\n",
        "\n",
        "\n",
        "\n",
        "class DecoderBlock(nn.Module):\n",
        "    \n",
        "    def __init__(self, dim, num_heads, expand, p):\n",
        "        super().__init__()\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "        self.attention = MultiHeadAttention(dim, num_heads, p)\n",
        "        self.transformer_block = TransformerBlock(dim, num_heads, expand, p)\n",
        "        self.dropout = nn.Dropout(p)\n",
        "\n",
        "    \n",
        "    def forward(self, x, key, value, src_mask=None, tgt_mask=None):\n",
        "        attention = self.attention(x, x, x, tgt_mask)\n",
        "        query = self.dropout(self.norm(attention + x))\n",
        "        out = self.transformer_block(query, key, value, src_mask)\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, dim, num_heads, expand, p):\n",
        "        super().__init__()\n",
        "        self.attention = MultiHeadAttention(dim, num_heads, p)\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.Linear(dim, expand*dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(p),\n",
        "            nn.Linear(expand*dim, dim)\n",
        "        )\n",
        "        self.attn_norm = nn.LayerNorm(dim)\n",
        "        self.ff_norm = nn.LayerNorm(dim)\n",
        "        self.dropout = nn.Dropout(p)\n",
        "    \n",
        "    \n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        # x = (N, len, dim)\n",
        "        # mask = (N, len)\n",
        "\n",
        "        # attn_out = (N, len, dim)\n",
        "        attn_out = self.attention(query, key, value, mask)\n",
        "        attn_out = self.dropout(attn_out)\n",
        "        attn_out = self.attn_norm(query + attn_out)\n",
        "\n",
        "        # ff_out = (N, len, dim)\n",
        "        ff_out = self.ff(attn_out)\n",
        "        ff_out = self.dropout(ff_out)\n",
        "        ff_out = self.ff_norm(attn_out + ff_out)\n",
        "\n",
        "        return ff_out\n",
        "\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, dim, num_heads, p):\n",
        "        assert dim % num_heads == 0\n",
        "        super().__init__()\n",
        "\n",
        "        self.dim = dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = dim // num_heads\n",
        "        scale = torch.rsqrt(torch.tensor(dim).float())\n",
        "        self.scale = nn.Parameter(scale, requires_grad=False)\n",
        "\n",
        "        self.Q = nn.Linear(dim, dim)\n",
        "        self.K = nn.Linear(dim, dim)\n",
        "        self.V = nn.Linear(dim, dim)\n",
        "        self.fc_out = nn.Linear(dim, dim)\n",
        "\n",
        "        self.dropout = nn.Dropout(p)\n",
        "\n",
        "\n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        # q,k,v: (N, len, dim)\n",
        "        N = query.shape[0]\n",
        "        query_len, key_len, value_len = query.shape[1], key.shape[1], value.shape[1]\n",
        "\n",
        "        query = self.Q(query)\n",
        "        key = self.K(key)\n",
        "        value = self.V(value)\n",
        "\n",
        "        # q,k,v: (N, len, num_heads, head_dim)\n",
        "        query = query.reshape(N, query_len, self.num_heads, self.head_dim)\n",
        "        key = key.reshape(N, key_len, self.num_heads, self.head_dim)\n",
        "        value = value.reshape(N, value_len, self.num_heads, self.head_dim)\n",
        "\n",
        "        # energy: (N, num_heads, query_len, key_len)\n",
        "        energy = torch.einsum('nqhd,nkhd->nhqk', [query, key]) * self.scale # really dividing, but used rsqrt\n",
        "\n",
        "        if mask is not None:\n",
        "            energy.masked_fill_(mask == 0, float('-inf'))\n",
        "\n",
        "        # attention: (N, heads, query_len, key_len)\n",
        "        attention = torch.softmax(energy, dim=-1)\n",
        "        attention = self.dropout(attention)\n",
        "\n",
        "        # out: (N, query_len, num_heads, head_dim)\n",
        "        out = torch.einsum('nhql,nlhd->nqhd', [attention, value])\n",
        "        \n",
        "        # out: (N, query_len, dim)\n",
        "        out = out.reshape(N, query_len, self.dim)\n",
        "        out = self.fc_out(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "class PosEmbedding(nn.Module):\n",
        "\n",
        "    def __init__(self, dim, seq_len, p):\n",
        "        super().__init__()\n",
        "        self.pos_embedding = nn.Embedding(seq_len, dim)\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "        self.dropout = nn.Dropout(p)\n",
        "        self.pos = torch.arange(0, seq_len).unsqueeze(0)\n",
        "        scale = torch.sqrt(torch.tensor(dim).float())\n",
        "        self.scale = nn.Parameter(scale, requires_grad=False)\n",
        "\n",
        "\n",
        "    def forward(self, seq):\n",
        "        N, seq_len, seq_dim = seq.shape\n",
        "        pos = self.pos.expand(N, seq_len).to(seq.device)\n",
        "        embeds = seq * self.scale + self.pos_embedding(pos)\n",
        "        return self.dropout(self.norm(embeds))\n",
        "\n",
        "\n",
        "class Embedding(nn.Module):\n",
        "\n",
        "    def __init__(self, dim, vocab_size, max_len, p):\n",
        "        super().__init__()\n",
        "        self.tok_embedding = nn.Embedding(vocab_size, dim)\n",
        "        self.pos_embedding = nn.Embedding(max_len, dim)\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "        self.dropout = nn.Dropout(p)\n",
        "        scale = torch.sqrt(torch.tensor(dim).float())\n",
        "        self.scale = nn.Parameter(scale, requires_grad=False)\n",
        "\n",
        "\n",
        "    def forward(self, seq):\n",
        "        # seq = (N, len)\n",
        "        N, seq_len = seq.shape\n",
        "        pos = torch.arange(0, seq_len).unsqueeze(0).expand(N, seq_len).to(seq.device)\n",
        "\n",
        "        # embeds = (N, len, dim)\n",
        "        embeds = self.tok_embedding(seq) * self.scale + self.pos_embedding(pos)\n",
        "        return self.dropout(self.norm(embeds))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQq88eT31ITz"
      },
      "source": [
        "def get_loss(outs, tgt):\n",
        "    return F.cross_entropy(\n",
        "        outs.reshape(-1, outs.shape[-1]),\n",
        "        tgt[:,1:].reshape(-1))\n",
        "    \n",
        "def get_accuracy(outs, tgt):\n",
        "    tgt = tgt[:,1]\n",
        "    outs = outs[:,0].argmax(-1)\n",
        "    return (tgt == outs).float().mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mkHyFYkRgS0B",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "54286df4-6e64-4dc4-9985-bdf5254b8e5b"
      },
      "source": [
        "batch_size = 512\n",
        "epochs = 50\n",
        "dim = 28\n",
        "src_vocab_size = None\n",
        "tgt_vocab_size = 12 # ten digits + sos + eos\n",
        "src_max_len = 28\n",
        "tgt_max_len = 3 # [sos, digit, eos]\n",
        "num_heads = 2\n",
        "num_layers = 2\n",
        "expand = 2\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "train_dl, valid_dl = get_data(batch_size)\n",
        "model = Seq2Seq(dim, \n",
        "                src_vocab_size, \n",
        "                tgt_vocab_size,\n",
        "                src_max_len,\n",
        "                tgt_max_len,\n",
        "                device,\n",
        "                num_heads,\n",
        "                num_layers,\n",
        "                expand).to(device)\n",
        "optimizer = optim.AdamW(model.parameters(), lr=1e-3)\n",
        "\n",
        "print(f'Model has {len(model)} parameters.')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model has 34244 parameters.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ULCKxicfeD3a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        },
        "outputId": "b16ecd81-9ac7-4349-fce2-0457dfa7e17d"
      },
      "source": [
        "sos = torch.tensor(10) # start token\n",
        "eos = torch.tensor(11) # end token\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    losses, accs = [], []\n",
        "\n",
        "    model.train()\n",
        "    for images, labels in train_dl:\n",
        "        images = images.to(device)\n",
        "        _sos = sos.expand(labels.shape[0])\n",
        "        _eos = eos.expand(labels.shape[0])\n",
        "        labels = torch.stack([_sos, labels, _eos]).t().to(device)\n",
        "        outs = model(images, labels[:,:-1])\n",
        "        loss = get_loss(outs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for images, labels in valid_dl:\n",
        "            images = images.to(device)\n",
        "            _sos = sos.expand(labels.shape[0])\n",
        "            _eos = eos.expand(labels.shape[0])\n",
        "            labels = torch.stack([_sos, labels, _eos]).t().to(device)\n",
        "            outs = model(images, labels[:,:-1])\n",
        "            loss = get_loss(outs, labels)\n",
        "            accuracy = get_accuracy(outs, labels)\n",
        "            losses.append(loss)\n",
        "            accs.append(accuracy)\n",
        "\n",
        "    loss = torch.stack(losses).mean().item()\n",
        "    acc = torch.stack(accs).mean().item()\n",
        "\n",
        "    print(f'Epoch: {epoch+1:>2}\\tLoss: {loss:.3f}\\tAccuracy: {acc:.3f}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch:  1\tLoss: 1.120\tAccuracy: 0.211\n",
            "Epoch:  2\tLoss: 0.920\tAccuracy: 0.339\n",
            "Epoch:  3\tLoss: 0.617\tAccuracy: 0.566\n",
            "Epoch:  4\tLoss: 0.376\tAccuracy: 0.758\n",
            "Epoch:  5\tLoss: 0.260\tAccuracy: 0.839\n",
            "Epoch:  6\tLoss: 0.220\tAccuracy: 0.862\n",
            "Epoch:  7\tLoss: 0.195\tAccuracy: 0.880\n",
            "Epoch:  8\tLoss: 0.172\tAccuracy: 0.893\n",
            "Epoch:  9\tLoss: 0.159\tAccuracy: 0.906\n",
            "Epoch: 10\tLoss: 0.136\tAccuracy: 0.917\n",
            "Epoch: 11\tLoss: 0.129\tAccuracy: 0.922\n",
            "Epoch: 12\tLoss: 0.116\tAccuracy: 0.930\n",
            "Epoch: 13\tLoss: 0.106\tAccuracy: 0.936\n",
            "Epoch: 14\tLoss: 0.103\tAccuracy: 0.938\n",
            "Epoch: 15\tLoss: 0.095\tAccuracy: 0.943\n",
            "Epoch: 16\tLoss: 0.090\tAccuracy: 0.946\n",
            "Epoch: 17\tLoss: 0.086\tAccuracy: 0.947\n",
            "Epoch: 18\tLoss: 0.085\tAccuracy: 0.948\n",
            "Epoch: 19\tLoss: 0.082\tAccuracy: 0.951\n",
            "Epoch: 20\tLoss: 0.080\tAccuracy: 0.951\n",
            "Epoch: 21\tLoss: 0.077\tAccuracy: 0.953\n",
            "Epoch: 22\tLoss: 0.074\tAccuracy: 0.955\n",
            "Epoch: 23\tLoss: 0.071\tAccuracy: 0.957\n",
            "Epoch: 24\tLoss: 0.069\tAccuracy: 0.958\n",
            "Epoch: 25\tLoss: 0.066\tAccuracy: 0.960\n",
            "Epoch: 26\tLoss: 0.067\tAccuracy: 0.959\n",
            "Epoch: 27\tLoss: 0.067\tAccuracy: 0.961\n",
            "Epoch: 28\tLoss: 0.062\tAccuracy: 0.963\n",
            "Epoch: 29\tLoss: 0.060\tAccuracy: 0.964\n",
            "Epoch: 30\tLoss: 0.059\tAccuracy: 0.964\n",
            "Epoch: 31\tLoss: 0.059\tAccuracy: 0.964\n",
            "Epoch: 32\tLoss: 0.059\tAccuracy: 0.964\n",
            "Epoch: 33\tLoss: 0.057\tAccuracy: 0.966\n",
            "Epoch: 34\tLoss: 0.055\tAccuracy: 0.967\n",
            "Epoch: 35\tLoss: 0.055\tAccuracy: 0.967\n",
            "Epoch: 36\tLoss: 0.053\tAccuracy: 0.969\n",
            "Epoch: 37\tLoss: 0.054\tAccuracy: 0.969\n",
            "Epoch: 38\tLoss: 0.051\tAccuracy: 0.970\n",
            "Epoch: 39\tLoss: 0.050\tAccuracy: 0.971\n",
            "Epoch: 40\tLoss: 0.051\tAccuracy: 0.971\n",
            "Epoch: 41\tLoss: 0.051\tAccuracy: 0.970\n",
            "Epoch: 42\tLoss: 0.050\tAccuracy: 0.971\n",
            "Epoch: 43\tLoss: 0.049\tAccuracy: 0.972\n",
            "Epoch: 44\tLoss: 0.047\tAccuracy: 0.973\n",
            "Epoch: 45\tLoss: 0.047\tAccuracy: 0.974\n",
            "Epoch: 46\tLoss: 0.047\tAccuracy: 0.974\n",
            "Epoch: 47\tLoss: 0.044\tAccuracy: 0.974\n",
            "Epoch: 48\tLoss: 0.046\tAccuracy: 0.973\n",
            "Epoch: 49\tLoss: 0.045\tAccuracy: 0.974\n",
            "Epoch: 50\tLoss: 0.045\tAccuracy: 0.974\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bxuk-dt58gjF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}